"""
Optunaè‡ªåŠ¨è°ƒå‚ç³»ç»Ÿ for CIFAR-10
æ”¯æŒSGD, Adam, AdamW, RMSpropå››ç§ä¼˜åŒ–å™¨çš„è¶…å‚æ•°ä¼˜åŒ–
"""
import torch
import argparse
import os
import sys
import optuna
from optuna.trial import TrialState
import json
from datetime import datetime, timezone, timedelta
import random
import numpy as np

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ç°åœ¨å¯ä»¥æ­£å¸¸å¯¼å…¥é¡¹ç›®æ¨¡å—
from utils.ops_al import create_model, get_optimizer_lr, get_optimizer_weight_decay, set_seed
from utils.ops_io import CIFAR10DataLoader
from utils.ops_tt import Trainer, Tester



class OptunaOptimizer:
    """Optunaä¼˜åŒ–å™¨å°è£…ç±»"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # ä½¿ç”¨åŒ—äº¬æ—¶é—´ (UTC+08:00)
        self.tz = timezone(timedelta(hours=8))
        self.start_time = datetime.now(self.tz)
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆåªåˆ›å»ºä¸€æ¬¡ï¼ŒèŠ‚çœæ—¶é—´ï¼‰
        print(f"\nåŠ è½½æ•°æ®é›†...")
        self.data_loader = CIFAR10DataLoader(
            data_dir=args.data_dir,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            use_cutout=args.use_cutout,
            validation_split=args.validation_split
        )
        self.train_loader, self.valid_loader = self.data_loader.get_train_valid_loader()
        print(f"è®­ç»ƒé›†å¤§å°: {len(self.train_loader.sampler)}")
        print(f"éªŒè¯é›†å¤§å°: {len(self.valid_loader.sampler)}")
        
        # åˆ›å»ºæµ‹è¯•é›†åŠ è½½å™¨ï¼ˆç”¨äºæœ€ç»ˆæµ‹è¯•è¯„ä¼°ï¼‰
        self.test_loader = self.data_loader.get_test_loader()
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        self.study_name = f"{args.optimizer}_optuna_{self.start_time.strftime('%Y%m%d_%H%M%S')}"
        self.save_dir = os.path.join(args.save_dir, self.study_name)
        os.makedirs(self.save_dir, exist_ok=True)
        
        # ä¿å­˜é…ç½®
        self._save_config()
    
    def _save_config(self):
        """ä¿å­˜å®éªŒé…ç½®"""
        config = {
            'optimizer': self.args.optimizer,
            'model': self.args.model,
            'epochs': self.args.epochs,
            'batch_size': self.args.batch_size,
            'n_trials': self.args.n_trials,
            'device': str(self.device),
            'timestamp': self.start_time.strftime('%Y-%m-%d %H:%M:%S')
        }
        with open(os.path.join(self.save_dir, 'config.json'), 'w') as f:
            json.dump(config, f, indent=4)
    
    def _get_param_ranges(self, trial, optimizer_name):
        """æ ¹æ®ä¼˜åŒ–å™¨ç±»å‹è·å–å‚æ•°æœç´¢èŒƒå›´"""
        params = {}
        
        if optimizer_name == 'sgd':
            # SGDå‚æ•°èŒƒå›´ï¼ˆå½“å‰96.9%ï¼Œå¾®è°ƒä»¥è¾¾åˆ°97%+ï¼‰
            params['learning_rate'] = trial.suggest_float('learning_rate', 0.05, 0.15, log=True)
            params['weight_decay'] = trial.suggest_float('weight_decay', 1e-4, 1e-3, log=True)
            params['mixup_alpha'] = trial.suggest_float('mixup_alpha', 0.2, 0.6)
            params['momentum'] = trial.suggest_float('momentum', 0.85, 0.95)
            params['label_smoothing'] = trial.suggest_float('label_smoothing', 0.05, 0.15)
            
        elif optimizer_name == 'adamw':
            # AdamWå‚æ•°èŒƒå›´ï¼ˆå½“å‰96.1%ï¼Œç›®æ ‡97%+ï¼‰
            params['learning_rate'] = trial.suggest_float('learning_rate', 0.0001, 0.005, log=True)
            params['weight_decay'] = trial.suggest_float('weight_decay', 1e-4, 1e-2, log=True)  # å…³é”®ï¼
            params['mixup_alpha'] = trial.suggest_float('mixup_alpha', 0.8, 1.5)  # å¼ºå¢å¼º
            params['beta1'] = trial.suggest_float('beta1', 0.85, 0.95)
            params['beta2'] = trial.suggest_float('beta2', 0.99, 0.9999)
            params['label_smoothing'] = trial.suggest_float('label_smoothing', 0.05, 0.2)
            
        elif optimizer_name == 'adam':
            # Adamå‚æ•°èŒƒå›´ï¼ˆå½“å‰94.6%ï¼Œç›®æ ‡96%+ï¼‰
            params['learning_rate'] = trial.suggest_float('learning_rate', 0.0001, 0.003, log=True)
            params['weight_decay'] = trial.suggest_float('weight_decay', 1e-5, 5e-4, log=True)
            params['mixup_alpha'] = trial.suggest_float('mixup_alpha', 0.5, 1.2)
            params['beta1'] = trial.suggest_float('beta1', 0.85, 0.95)
            params['beta2'] = trial.suggest_float('beta2', 0.99, 0.9999)
            params['label_smoothing'] = trial.suggest_float('label_smoothing', 0.05, 0.15)
            
        elif optimizer_name == 'rmsprop':
            # RMSpropå‚æ•°èŒƒå›´ï¼ˆå½“å‰94.5%ï¼Œç›®æ ‡96%+ï¼‰
            params['learning_rate'] = trial.suggest_float('learning_rate', 0.0001, 0.005, log=True)
            params['weight_decay'] = trial.suggest_float('weight_decay', 1e-5, 5e-4, log=True)
            params['mixup_alpha'] = trial.suggest_float('mixup_alpha', 0.4, 1.0)
            params['alpha'] = trial.suggest_float('alpha', 0.9, 0.999)
            params['momentum'] = trial.suggest_float('momentum', 0.0, 0.9)
            params['label_smoothing'] = trial.suggest_float('label_smoothing', 0.05, 0.15)
        
        return params
    
    def objective(self, trial):
        """Optunaçš„ç›®æ ‡å‡½æ•°"""
        # è®¾ç½®éšæœºç§å­ï¼ˆæ¯æ¬¡trialä½¿ç”¨ä¸åŒçš„ç§å­ä»¥é¿å…è¿‡æ‹Ÿåˆï¼‰
        seed = self.args.seed + trial.number
        set_seed(seed)
        
        # è·å–è¶…å‚æ•°
        params = self._get_param_ranges(trial, self.args.optimizer)
        
        # æ‰“å°å½“å‰trialçš„å‚æ•°
        print(f"\n{'='*80}")
        print(f"Trial {trial.number + 1}/{self.args.n_trials}")
        print(f"{'='*80}")
        print("Parameters:")
        for key, value in params.items():
            print(f"  {key}: {value}")
        print(f"{'='*80}\n")
        
        # å¤„ç†GPUè®¾å¤‡
        if hasattr(self.args, 'gpu_ids') and self.args.gpu_ids:
            gpu_ids = [int(x.strip()) for x in self.args.gpu_ids.split(',')]
            device = torch.device(f'cuda:{gpu_ids[0]}')
        else:
            device = self.device
            gpu_ids = None
        
        # åˆ›å»ºæ¨¡å‹ï¼ˆä¼ å…¥æ­£ç¡®çš„å‚æ•°ï¼‰
        dropout_rate = params.get('dropout', self.args.dropout if hasattr(self.args, 'dropout') else 0.0)
        model = create_model(
            model_name=self.args.model,
            num_classes=10,
            dropout_rate=dropout_rate,
            device=device
        )
        
        # å¤šGPUæ”¯æŒ
        if self.args.multi_gpu and torch.cuda.device_count() > 1:
            if gpu_ids:
                print(f"ä½¿ç”¨æŒ‡å®šGPU: {gpu_ids}")
                model = torch.nn.DataParallel(model, device_ids=gpu_ids)
            else:
                print(f"ä½¿ç”¨æ‰€æœ‰GPU: {torch.cuda.device_count()} ä¸ª")
                model = torch.nn.DataParallel(model)
        
        # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆéœ€è¦ä¿®æ”¹ä»¥æ”¯æŒåŠ¨æ€å‚æ•°ï¼‰
        trainer = self._create_trainer(model, params, device)
        
        # è®­ç»ƒæ¨¡å‹
        try:
            best_val_acc = trainer.train(save_path=self.save_dir)
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ï¼Œç”¨æµ‹è¯•é›†å‡†ç¡®ç‡ä½œä¸ºoptunaçš„ä¼˜åŒ–ç›®æ ‡
            test_acc = self._test_on_test_set(model, device)
            
            print(f"\nğŸ“Š Trial {trial.number} ç»“æœ:")
            print(f"  éªŒè¯é›†æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.4f}%")
            print(f"  æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}%")
            print(f"  éªŒè¯-æµ‹è¯•å·®å¼‚: {best_val_acc - test_acc:.4f}%")
            
            # å¦‚æœå·®å¼‚è¿‡å¤§ï¼Œå‘å‡ºè­¦å‘Š
            if best_val_acc - test_acc > 2.0:  # å·®å¼‚è¶…è¿‡2%
                print(f"  âš ï¸  è­¦å‘Š: éªŒè¯é›†å’Œæµ‹è¯•é›†æ€§èƒ½å·®å¼‚è¾ƒå¤§ï¼Œå¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ!")
            
            # ä¿å­˜trialç»“æœ
            trial_result = {
                'trial_number': trial.number,
                'params': params,
                'validation_accuracy': best_val_acc,
                'test_accuracy': test_acc,
                'val_test_gap': best_val_acc - test_acc,
                'seed': seed
            }
            
            result_file = os.path.join(self.save_dir, f'trial_{trial.number}.json')
            with open(result_file, 'w') as f:
                json.dump(trial_result, f, indent=4)
            
            # ğŸ”‘ å…³é”®ï¼è¿”å›æµ‹è¯•é›†å‡†ç¡®ç‡ä½œä¸ºoptunaçš„ä¼˜åŒ–ç›®æ ‡
            return test_acc
            
        except Exception as e:
            print(f"\nTrial {trial.number} å¤±è´¥: {str(e)}")
            return 0.0
    
    def _create_trainer(self, model, params, device):
        """åˆ›å»ºè®­ç»ƒå™¨ï¼Œæ”¯æŒåŠ¨æ€å‚æ•°"""
        # é‡ç”¨æ­£å¼è®­ç»ƒçš„å‚æ•°è®¾ç½®é€»è¾‘
        trainer = Trainer(
            model=model,
            train_loader=self.train_loader,
            valid_loader=self.valid_loader,
            device=device,
            optimizer_name=self.args.optimizer,
            learning_rate=params['learning_rate'],
            weight_decay=params['weight_decay'],
            epochs=self.args.epochs,
            use_mixup=self.args.use_mixup,
            mixup_alpha=params['mixup_alpha'],
            label_smoothing=params['label_smoothing'],
            scheduler_type=self.args.scheduler,
            log_file=None  # optunaä¸éœ€è¦è¯¦ç»†æ—¥å¿—
        )
        
        # å¯¹äºæœ‰é¢å¤–å‚æ•°çš„ä¼˜åŒ–å™¨ï¼Œéœ€è¦é‡æ–°åˆ›å»ºä¼˜åŒ–å™¨ä»¥æ”¯æŒoptunaåŠ¨æ€è°ƒå‚
        if self.args.optimizer == 'sgd' and 'momentum' in params:
            trainer.optimizer = torch.optim.SGD(
                model.parameters(),
                lr=params['learning_rate'],
                momentum=params['momentum'],
                weight_decay=params['weight_decay'],
                nesterov=True
            )
        elif self.args.optimizer in ['adam', 'adamw'] and 'beta1' in params:
            optimizer_class = torch.optim.AdamW if self.args.optimizer == 'adamw' else torch.optim.Adam
            trainer.optimizer = optimizer_class(
                model.parameters(),
                lr=params['learning_rate'],
                betas=(params['beta1'], params['beta2']),
                weight_decay=params['weight_decay'],
                eps=1e-8
            )
        elif self.args.optimizer == 'rmsprop' and 'alpha' in params:
            trainer.optimizer = torch.optim.RMSprop(
                model.parameters(),
                lr=params['learning_rate'],
                alpha=params['alpha'],
                momentum=params.get('momentum', 0),
                weight_decay=params['weight_decay'],
                eps=1e-8
            )
        
        # é‡æ–°åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
        if self.args.scheduler == 'cosine':
            trainer.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                trainer.optimizer, T_max=self.args.epochs, eta_min=1e-6
            )
        elif self.args.scheduler == 'multistep':
            trainer.scheduler = torch.optim.lr_scheduler.MultiStepLR(
                trainer.optimizer, milestones=[60, 120, 160], gamma=0.2
            )
        
        return trainer
    
    def _test_on_test_set(self, model, device):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ - é‡ç”¨æ ‡å‡†Testerç±»"""
        tester = Tester(model=model, test_loader=self.test_loader, device=device)
        test_acc = tester.test()
        return test_acc
    
    def optimize(self):
        """æ‰§è¡Œä¼˜åŒ–"""
        print(f"\n{'='*80}")
        print(f"å¼€å§‹Optunaä¼˜åŒ– - {self.args.optimizer.upper()}")
        print(f"{'='*80}")
        print(f"ä¼˜åŒ–å™¨: {self.args.optimizer}")
        print(f"æ¨¡å‹: {self.args.model}")
        print(f"è®­ç»ƒè½®æ•°: {self.args.epochs}")
        print(f"æ€»Trialæ•°: {self.args.n_trials}")
        print(f"ä¿å­˜ç›®å½•: {self.save_dir}")
        print(f"{'='*80}\n")
        
        # åˆ›å»ºOptuna study
        study = optuna.create_study(
            direction='maximize',
            study_name=self.study_name,
            sampler=optuna.samplers.TPESampler(seed=self.args.seed)
        )
        
        # è¿è¡Œä¼˜åŒ–
        study.optimize(
            self.objective,
            n_trials=self.args.n_trials,
            show_progress_bar=False  # ç¦ç”¨optunaè¿›åº¦æ¡ï¼Œé¿å…ä¸è®­ç»ƒè¿›åº¦æ¡å†²çª
        )
        
        # ä¿å­˜ç»“æœ
        self._save_results(study)
        
        # æ‰“å°æœ€ä½³ç»“æœ
        self._print_results(study)
        
        return study
    
    def _save_results(self, study):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        # ä¿å­˜æœ€ä½³å‚æ•°
        best_params = study.best_params
        best_value = study.best_value
        
        results = {
            'best_accuracy': best_value,
            'best_params': best_params,
            'best_trial_number': study.best_trial.number,
            'n_trials': len(study.trials),
            'completed_trials': len([t for t in study.trials if t.state == TrialState.COMPLETE])
        }
        
        with open(os.path.join(self.save_dir, 'best_results.json'), 'w') as f:
            json.dump(results, f, indent=4)
        
        # ä¿å­˜æ‰€æœ‰trialsçš„å†å²
        trials_data = []
        for trial in study.trials:
            if trial.state == TrialState.COMPLETE:
                trials_data.append({
                    'number': trial.number,
                    'value': trial.value,
                    'params': trial.params
                })
        
        with open(os.path.join(self.save_dir, 'all_trials.json'), 'w') as f:
            json.dump(trials_data, f, indent=4)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {self.save_dir}")
    
    def _print_results(self, study):
        """æ‰“å°ä¼˜åŒ–ç»“æœ"""
        print(f"\n{'='*80}")
        print("ä¼˜åŒ–å®Œæˆ!")
        print(f"{'='*80}")
        print(f"æœ€ä½³å‡†ç¡®ç‡: {study.best_value:.4f}%")
        print(f"æœ€ä½³Trial: {study.best_trial.number}")
        print("\næœ€ä½³å‚æ•°:")
        for key, value in study.best_params.items():
            print(f"  {key}: {value}")
        print(f"{'='*80}\n")
        
        # æ‰“å°Top 5 trials
        sorted_trials = sorted(
            [t for t in study.trials if t.state == TrialState.COMPLETE],
            key=lambda t: t.value,
            reverse=True
        )[:5]
        
        print("Top 5 Trials:")
        for i, trial in enumerate(sorted_trials, 1):
            print(f"\n{i}. Trial {trial.number}: {trial.value:.4f}%")
            print("   Parameters:")
            for key, value in trial.params.items():
                print(f"     {key}: {value}")


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Optunaè¶…å‚æ•°ä¼˜åŒ– for CIFAR-10')
    
    # Optunaå‚æ•°
    parser.add_argument('--optimizer', type=str, default='adamw',
                        choices=['sgd', 'adam', 'adamw', 'rmsprop'],
                        help='è¦ä¼˜åŒ–çš„ä¼˜åŒ–å™¨')
    parser.add_argument('--n_trials', type=int, default=50,
                        help='Optunaè¯•éªŒæ¬¡æ•°ï¼ˆå»ºè®®è‡³å°‘30æ¬¡ï¼‰')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', type=str, default='wideresnet28_10',
                        choices=['wideresnet28_10', 'wideresnet40_10'],
                        help='æ¨¡å‹æ¶æ„')
    parser.add_argument('--dropout', type=float, default=0.0,
                        help='Dropoutç‡')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=200,
                        help='æ¯ä¸ªtrialçš„è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=128,
                        help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--scheduler', type=str, default='cosine',
                        choices=['cosine', 'multistep'],
                        help='å­¦ä¹ ç‡è°ƒåº¦å™¨')
    
    # æ•°æ®å¢å¼ºå‚æ•°
    parser.add_argument('--use_mixup', action='store_true', default=True,
                        help='ä½¿ç”¨Mixupæ•°æ®å¢å¼º')
    parser.add_argument('--use_cutout', action='store_true', default=True,
                        help='ä½¿ç”¨Cutoutæ•°æ®å¢å¼º')
    
    # æ•°æ®å‚æ•°  
    parser.add_argument('--data_dir', type=str, 
                        default=os.path.join(project_root, 'data'),
                        help='æ•°æ®ç›®å½• (é»˜è®¤: é¡¹ç›®æ ¹ç›®å½•/data, å¯è‡ªå®šä¹‰ç»å¯¹è·¯å¾„)')
    parser.add_argument('--num_workers', type=int, default=4,
                        help='æ•°æ®åŠ è½½çº¿ç¨‹æ•°')
    parser.add_argument('--validation_split', type=float, default=0.1,
                        help='éªŒè¯é›†æ¯”ä¾‹')
    
    # å¤šGPUå‚æ•°
    parser.add_argument('--multi_gpu', action='store_true', default=True,
                        help='ä½¿ç”¨å¤šGPUè®­ç»ƒ')
    parser.add_argument('--gpu_ids', type=str, default=None,
                        help='æŒ‡å®šä½¿ç”¨çš„GPU IDï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚"0,1,2,3"')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--save_dir', type=str, 
                        default=os.path.join(project_root, 'optuna_results'),
                        help='ç»“æœä¿å­˜ç›®å½• (é»˜è®¤: é¡¹ç›®æ ¹ç›®å½•/optuna_results, å¯è‡ªå®šä¹‰ç»å¯¹è·¯å¾„)')
    parser.add_argument('--seed', type=int, default=1009,
                        help='éšæœºç§å­')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = OptunaOptimizer(args)
    
    # æ‰§è¡Œä¼˜åŒ–
    study = optimizer.optimize()
    
    print("\nä¼˜åŒ–å®Œæˆ! æŸ¥çœ‹ç»“æœ:")
    print(f"  ä¿å­˜ç›®å½•: {optimizer.save_dir}")
    print(f"  æœ€ä½³å‡†ç¡®ç‡: {study.best_value:.4f}%")


if __name__ == '__main__':
    main()
